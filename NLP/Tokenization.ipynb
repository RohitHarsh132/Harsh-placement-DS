{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization** is the process of splitting text into smaller units called tokens"
      ],
      "metadata": {
        "id": "iIHGUGDwuwgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Why is Tokenization Important?**\n",
        "\n",
        "Machines don't understand language; they understand numbers. Tokenization helps convert human language into units that machines can process for:\n",
        "\n",
        "Text classification\n",
        "\n",
        "Sentiment analysis\n",
        "\n",
        "Machine translation"
      ],
      "metadata": {
        "id": "pX-zcGjyvE6s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1g8l1IHCsjie"
      },
      "outputs": [],
      "source": [
        "!pip install nltk spacy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa9VlK1gs2cX",
        "outputId": "bdc1264e-6495-4d86-e606-15b751d06201"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANxZ0FS9uD7m",
        "outputId": "a81f2a45-f401-44e7-b802-1561ac8b6932"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "N3a7t705tA4v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"Hello! My name is Harsh. I love coding and solving real-world problems using data.\"\n",
        "print(\"Original Text:\\n\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owgdsMGrtDXA",
        "outputId": "d93dc432-8df3-4aae-c0db-50fd5f01e7c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " Hello! My name is Harsh. I love coding and solving real-world problems using data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK word Tokenization**\n"
      ],
      "metadata": {
        "id": "AoOb3nMXtS72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_tokenize is pre trained tokenizer from NLTK .\n",
        "it split \"Harsh.\" into \"Harsh\" and \".\""
      ],
      "metadata": {
        "id": "05Is9Ox0vkf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import  word_tokenize\n",
        "nltk_token=word_tokenize(text)\n",
        "print(\"word tokens : \\n\",nltk_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7JVpGSmtNun",
        "outputId": "89849ae5-6500-4de0-aa8e-c34ed1778304"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word tokens : \n",
            " ['Hello', '!', 'My', 'name', 'is', 'Harsh', '.', 'I', 'love', 'coding', 'and', 'solving', 'real-world', 'problems', 'using', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spacy word and Sentence Tokenization**"
      ],
      "metadata": {
        "id": "xuWGS7f3v8zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(text)\n",
        "spacy_token=[token.text for token in doc]\n",
        "spacy_sentence_token=[sent.text for sent in doc.sents]\n",
        "print(\"word tokens : \\n\",spacy_token)\n",
        "print(\"sentence tokens : \\n\",spacy_sentence_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMg8xUKXtfbj",
        "outputId": "08b43d37-52bd-4711-c40e-ae5ec941146e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word tokens : \n",
            " ['Hello', '!', 'My', 'name', 'is', 'Harsh', '.', 'I', 'love', 'coding', 'and', 'solving', 'real', '-', 'world', 'problems', 'using', 'data', '.']\n",
            "sentence tokens : \n",
            " ['Hello!', 'My name is Harsh.', 'I love coding and solving real-world problems using data.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tPU6QY9lwZTf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}